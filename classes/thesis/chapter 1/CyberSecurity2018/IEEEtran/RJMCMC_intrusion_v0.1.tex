\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{dblfloatfix}

%\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{An Intrusion Detection Model based on Asymmetric Gaussian mixtures with Reversible Jump MCMC\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Shuai Fu}
\IEEEauthorblockA{\textit{Faculty of Engineering and Computer Science}\\
\textit{Concordia University}\\
Montreal, Canada\\
f\_shuai@encs.concordia.ca}
\and
\IEEEauthorblockN{Nizar Bouguila}
\IEEEauthorblockA{\textit{Concordia Institute for Information Systems Engineering} \\
\textit{Concordia University}\\
Montreal, Canada \\
bouguila@ciise.concordia.ca}
}

\maketitle

\begin{abstract}
This paper summarizes our work on a novel intrusion detection classifier based on asymmetric Gaussian mixtures (AGM) model and reversible jump Markov chain Monte Carlo (RJMCMC) learning algorithm. Previous efforts reveal the fact that AGM overperforms classic Gaussian mixture model (GMM) by taking asymmetric datasets into consideration which provides more flexibility. Our RJMCMC implementation is based on a hybrid sampling-based approach which takes advantages of both Metropolis-Hastings (MH) and Gibbs sampling methods, therefore, simplifies mathematical complexity and extends adaptability of the model. Moreover, without giving a fixed components number in advance, RJMCMC applies a dynamic data-based strategy to identify the optimal components number throughout iterations which makes the model learning a self-adaptive process. Since the model is nondeterministic, Laplace approximation based marginal likelihood will be calculated for multiple runs as model selection procedure to improve the correctness and fitting accuracy. Both synthetic and challenging intrusion detection datasets are applied to our model to discover its merits.
\end{abstract}

\begin{IEEEkeywords}
Asymmetric Gaussian Mixture, Metropolis-Hastings, Gibbs Sampling, RJMCMC, Laplace Approximation, Intrusion Detection
\end{IEEEkeywords}

\section{Introduction}
Along with the rapid growth of information technologies, personal and commercial behaviors tend to rely on computer network and Internet environments. However, based on the characteristics of networking, exposing sensitive privacy and valuable business secret online is extremely dangerous because accessibility and anonymity make network intrusions hard to be detected and traced, therefore, compromis network security. Cisco 2017 Annual Cybersecurity Report (ACR) \cite{Cisco2017} pointed out a crucial fact that more than one-third of organizations that experienced a breach in 2016 reported more than 20 percent of customer, opportunity and revenue loss. As a consequence, more than 90 percent of these organizations are improving threat defense technologies and processes by enhancing IT and security functions, increasing security training of employees and implementing risk mitigation techniques. Recently, machine learning-based intrusion detection solutions are drawing more attention because of its efficiency and flexibility. 

Earlier intrusion prevention approaches, such as authentication, avoiding programming errors and encryption, were proven as insufficient because along with the increasing of the complexity of network-based software system, exploitable weaknesses are inevitable due to programming issues. Moreover, authentication and encryption are not always reliable since credentials could be leaked and encryption algorithm could also be compromised by applying powerful hacking technique and hardware to make the attack computational feasible. In consequence, once intrusion happens, detection will be harder than prevention and sometimes victims could not be even aware of it. Therefore, many supervised data mining solutions were proposed in terms of misuse and anomaly detection systems by establishing known intrusion scenarios, normal usage patterns and the sequential interrelations between user operations to identify intrusion behaviors \cite{Lee1998}. However, the disadvantages of supervised intrusion detection systems are significant since predefined patterns and interrelations are inconsistent concerning the system upgrades and newly-founded intrusions which could lead to incessant intrusion detection system adjustment and affect its performance. Furthermore, inductive bias and overfitting problems caused by poor training datasets will also affect the accuracy of the systems. Therefore, researchers are paying more attention to unsupervised solution for seeking flexibility and robustness.

As an improvement of independent methodologies based on single mathematical model, mixture models can be seen as a superimposition of certain mixture components having dependencies with each other, therefore, provide outstanding suitability and generality especially for adapting multidimensional datasets. Particularly, Gaussian mixture model (GMM)\cite{Richardson1997} demonstrated effective learning abilities in several regions such as computer vision, pattern recognition and data mining, especially for application whose datasets are Gaussian-like. However, under a more general circumstances regarding to non-Gaussian or asymmetric datesets, asymmetric Gaussian mixture (AGM) model \cite{Elguebaly2014} is a better choice because its two variance parameters related to left and right parts of each component respectively in the mixture model, bringing better accuracy of fitting real applications. Therefore, we choose AGM as intrusion detection model and its merits will be discussed in detail throughout this paper.

One of the most challenging tasks of applying mixture model is the parameter learning process. It could be achieved by applying maximum-likelihood based expectation maximization (EM) algorithm \cite{Dempster1977} which is widely deployed and be proven as an effective estimation approach. However, as a deterministic solution, bad initialization and overfitting problems \cite{Bouguila2009} \cite{Bouguila2012} will significantly affect its efficiency and usability by causing slow convergence and improper approximation, eventually, compromise the accuracy of the model. Accordingly, in order to improve the parameter estimation, sampling-based stochastic Bayesian learning algorithms, such as Markov Chain Monte Carlo (MCMC) based implementations, are proposed to address overfitting problems by introducing prior and posterior distributions for every mixture parameter which decouple the dependency between mixture parameters and mixture components, therefore, allow model adjustment by substituting proposed distributions to adapt to varied application datasets. In this paper, the base learning algorithm is chosen as a hybrid MCMC implementation, which is well known as Metropolis-Hastings within Gibbs sampling\cite{Bouguila2009}, based on both Metropolis-Hastings \cite{Hastings1970} and Gibbs sampling \cite{Geman1987} methods because the main difficulty of applying traditional MCMC method is that, under some circumstances, direct sampling is not always straightforward that distributions of mixture parameters are latent and dependencies between parameters are unknown. By taking the advantages of both methods into consideration, proposal mixture parameters will be generated iteratively and decisions will be made by calculating the acceptance. Eventually, the optimal parameter values will be identified after convergence. Furthermore, we extend the learning algorithm by introducing reversible jump MCMC (RJMCMC)\cite{Bouguila2012} concept where the mixture components number will be treated as an extra parameter which could be variant throughout iterations by increasing (component birth/death step) and decreasing (component merge/split step) mixture components, therefore, enable model transfer which significantly improves the learning flexibility of the AGM model. On the other hand, because of this stochastic learning process, the iterations could end up with different models by setting different initial components numbers. In order to evaluate the learning performances and identify the best-fit result, the model selection will be performed by calculating marginal likelihood \cite{Bouguila2009}.

The next chapters are organized as follows. Section II elaborates the Bayesian model and learning algorithms. Section III is concentrating on experimental results analysis and Section IV gives an conclusion of the paper and proposes future research directions.


\section{Bayesian model}
\subsection{Asymmetric Gaussian Mixture Model}
The likelihood function of AGM model\cite{Elguebaly2014} with $M$ mixture components can be illustrated as follows:

\begin{align}
p(\mathcal{X}|\Theta) = \prod_{i=i}^N \sum_{j=1}^Mp_jp(X_i|\xi_j)
\label{eq:likelihood}
\end{align}

where $\mathcal{X} = (X_1,...,X_N)$ reprensents the dataset with $N$ observations, $\Theta = \{p_1,...,p_M, \xi_1,...,\xi_M\}$ defines the mxiture parameters set of AGM mixture model including component weight $p_j$ (0 $< p_j \leq$ 1 and $\sum_{j=1}^Mp_j$ = 1) and asymmetric Gaussian distribution (AGD) parameters set $\xi_j$ for mixture component $j$. Assuming the dataset $\mathcal{X}$ is $d$-dimensional, for each observation $X_n = (x_{n1},...,x_{nd})\in\mathcal{X}$, the probability density function\cite{Elguebaly2014} for $j$-th component of the model can be defined as follows:

\begin{multline}
p(X_n|\xi_j) \propto \prod_{k=1}^{d} \frac{1}{(\sigma_{l_{jk}}+\sigma_{r_{jk}})} \\
\times \left\{\begin{matrix}
\exp \begin{bmatrix}
-\frac{(x_{nk}-\mu_{jk})^2}{2(\sigma_{l_{jk}})^2}
\end{bmatrix}\ if\ x_{nk}\ <\ \mu_{jk} \\ 
\exp \begin{bmatrix}
-\frac{(x_{nk}-\mu_{jk})^2}{2(\sigma_{r_{jk}})^2}
\end{bmatrix}\ if\ x_{nk}\ \geqslant\ \mu_{jk} \\ 
\end{matrix}\right.
\label{eq:pdf}
\end{multline}

parameters set of component $j$ is $\xi_j = (\mu_j,\sigma_{lj},\sigma_{rj})$ where $\mu_j = (\mu_{j1},...,\mu_{jd})$ is the mean, $\sigma_{lj} = (\sigma_{lj1},...,\sigma_{ljd})$ and $\sigma_{rj} = (\sigma_{rj1},...,\sigma_{rjd})$ represents the left and right standard deviation vectors of AGD . 

We bring a $M$-dimensional membership vector $Z$ to each observation $X_i\in\mathcal{X}, Z_i = (Z_{i1},...,Z_{iM})$, indicating which specific component $X_i$ belongs to\cite{Bouguila2006}, such that:
\begin{align}
Z_{ij} = \left\{\begin{matrix}
1\qquad\mbox{ if }X_i\mbox{  belongs to component }j \\
0\qquad\quad\qquad \mbox{otherwise} \qquad\qquad\quad\quad \\
\end{matrix}\right.
\label{eq:memVector}
\end{align}
that being said, $Z_{ij} = 1$ only when observation $X_i$ has the highest probability of belonging to component $j$ and accordingly, for other components, $Z_{ij} = 0$. 

Hence, the complete likelihood function can be obtained by combining Eq. \eqref{eq:likelihood} and Eq. \eqref{eq:memVector} as follows:
\begin{align}
p(\mathcal{X}, Z|\Theta) = \prod_{i=1}^{N}\prod_{j=1}^{M}(p_jp(X_i|\xi_j))^{Z_{ij}}
\label{eq:compPdf}
\end{align}


\subsection{Priors and Posteriors}
As mentioned before, MH-within-Gibbs based RJMCMC learning algorithm implementation introduces definitions of priors and posteriors for mixture weighs and parameters to avoid direct sampling. For a specific iteration $t$, since mixture weight $p_j$ satisfies 0 $< p_j \leq$ 1 and $\sum_{j=1}^Mp_j$ = 1, a nature choice of the prior is Dirichlet distribution\cite{Elguebaly2013} as follows:
\begin{align}
\pi(p_j^{(t)}) \sim \mathcal{D}(\gamma_1,...,\gamma_M )
\label{eq:priorWeight}
\end{align}

where $\gamma = \gamma_j = \dots = \gamma_M$ is a known hyperparameter. By taking the membership vector Z into account as a condition, the posterior probability of mixture weight $p_j$ is defined as follows:
\begin{align}
p(p_j^{(t)}|Z^{(t)}) \sim \mathcal{D}(\gamma_1 + n_1^{(t)},...,\gamma_M + n_M^{(t)})
\label{eq:posterWeight}
\end{align}

where $n_j$ represents the number of observations of component $j$ which could be calculated using membership vectors as follows:
\begin{align}
n_j^{(t)} = \sum_{i=1}^NZ_{ij}\ (j = 1,...,M) 
\label{eq:nj}
\end{align}

The sampling process of mixture parameters employs the same concept. The proposal posterior distribution is  $\xi^{(t)} \sim q(\xi|\xi^{(t-1)})$. More specifically, for parameters of AGM model $\xi^{(t)} = (\mu^{(t)}, \sigma_{l}^{(t)}, \sigma_{r}^{(t)})$, we choose $d$-dimensional Gaussian distributions as posterior distributions respectively:
\begin{multline}
\qquad\qquad\qquad\qquad\mu_j^{(t)} \sim \mathcal{N}_d(\mu_j^{(t-1)},\Sigma) \\
\qquad\sigma_{lj}^{(t)} \sim \mathcal{N}_d(\sigma_{lj}^{(t-1)},\Sigma) \\
\sigma_{rj}^{(t)} \sim \mathcal{N}_d(\sigma_{rj}^{(t-1)},\Sigma)\qquad\qquad\quad
\label{eq:posters}
\end{multline}

where $\Sigma$ is $d$ x $d$ identity matrix which makes the sampling a random walk MCMC process. Correspondingly, the priors are $\mu \sim \mathcal{N}_d(\eta,\Sigma)$ and $\sigma_l, \sigma_r \sim \mathcal{N}_d(\tau,\Sigma)$ given known hyperparameters $\eta$ and $\tau$.


\subsection{Learning Algorithm}
\subsubsection*{MH-within-Gibbs}
MH-within-Gibbs method, a sampling-based learning algorithm,  performs random sampling from posteriors of parameters, and then calculate the acceptance ratio $r$ in order to make a decision whether the new samples should be accepted or discarded for next iteration. Due to the usage of membership vector $Z$, the mixture weight $p_j$ can be derived within Gibbs sampling part. Therefore, it will be excluded from the calculation of the acceptance ratio $r$ which is defined as follows:
\begin{align}
r = \frac{p(\mathcal{X}|\Theta^{(t)})\pi(\Theta^{(t)})q(\Theta^{(t-1)}|\Theta^{(t)})}{p(\mathcal{X}|\Theta^{(t-1)})\pi(\Theta^{(t-1)})q(\Theta^{(t)}|\Theta^{(t-1)})}
\label{eq:r}
\end{align}

Further information about the calculation of acceptance ratio
$r$ is explained in Appendix A. Once acceptance ratio $r$ is derived, acceptance probability $\alpha = min[1,r]$ \cite{Luengo2013} could be computed. Then $u \sim U_{[0,1]}$ is supposed to be generated randomly. If $\alpha < u$, the proposed move should be accepted and parameters should be updated by $p^{(t)}$ and $\xi^{(t)}$ for next iteration. Otherwise, we discard $p^{(t)}$, $\xi^{(t)}$ and set $p^{(t)} = p^{(t-1)}$, $\xi^{(t)} = \xi^{(t-1)}$. 
\bigskip

\begin{figure}[b]
\centering
\includegraphics[width=0.4\paperwidth]{DAG_RJMCMC.jpg}
\caption{DAG of RJMCMC parameter learning Bayesian network}
\label{fig:1}
\end{figure}

\subsubsection*{RJMCMC moves}
Traditional MH-within-Gibbs algorithm assumes that the components number $M$ is given and persistent throughout the learning process. However, because of bad initialization or just information leakage, components number $M$ could be inaccurate or unknown. Under these circumstances, RJMCMC algorithm presents its merits by providing extra four independent steps (birth/death steps and merge/split steps) into learning process which could change components number $M$, therefore, brings more generalities. 

In practice, within every RJMCMC learning iteration, the current components number $m$ is considered as an extra parameter which has a proposed Poisson prior $\mathcal{P}(\lambda)$ with $\lambda = 4$ particularly in our case \cite{Richardson1997}. Accordingly, letting $M_{min}$ and $M_{max}$ denote the minimum and maximum number of components $M$, assuming the probabilities of performing birth/split and death/merge steps are $b_m$ and $d_m = 1 - b_m$ for $m = M_{min},\dots,M_{max}$ respectively. Obviously, $b_{M_{max}}=0$ and $d_{M_{min}}=0$. Correspondingly, $d_{M_{max}}=1-b_{M_{max}} = 1$ and $b_{M_{min}}=1-d_{M_{min}}=1$. For $m=M_{min}+1,\dots,M_{max}-1$, for simplification purpose, we choose the same value for both $b_m$ and $d_m$ as $b_m=d_m=0.5$. Within every iteration, we generate a random value $u' \sim U_{[0,1]}$ respectively for the four RJMCMC steps. If $b_m >= u'$ or $d_m >= u'$, birth/split or death/merge steps should be performed correspondingly.\cite{Richardson1997}

\textbf{Merge and Split Steps}: Randomly choose two components $(j_1,j_2)$ satisfying that $\mu_{j_1}<\mu_{j_2}$ with no other $\mu_j$ in the interval $[\mu_{j_1},\mu_{j_2}]$. The newly merged component $j'$ will contain the observations that previously belonged to both component $j_1$ and $j_2$. Meanwhile, reduce current value of components number $m$ to $m-1$, then calculate mixture weight and parameters for $j'$ as follows:
\begin{multline}
\qquad\qquad\qquad\qquad p_{j'} = p_{j_1}+p_{j_2} \\
p_{j'}\mu_{j'} = p_{j_1}\mu_{j_1} + p_{j_2}\mu_{j_2} \\
p_{j'}(\mu_{j'}^2 + \sigma_{j'l}^2) = p_{j_1}(\mu_{j_1}^2 + \sigma_{j_1l}^2)\qquad \\
\qquad\qquad+ p_{j_1}(\mu_{j_1}^2 + \sigma_{j_1l}^2) \\
p_{j'}(\mu_{j'}^2 + \sigma_{j'r}^2) = p_{j_1}(\mu_{j_1}^2 + \sigma_{j_1r}^2)\qquad\\
+p_{j_1}(\mu_{j_1}^2 + \sigma_{j_1r}^2)\qquad\qquad
\label{eq:merge}
\end{multline}

As a reverse of merge step, we split component $j'$ into two ($j_1$ and $j_2$) with 3 degrees of freedom $(u_1 \sim Beta(2,2), u_2 \sim Beta(2,2), u_3 \sim Beta(1,1))$ and, accordingly, increase $m$ to $m+1$. Therefore, mixture parameters for split components can be calculated as follows:
\begin{multline}
\qquad\qquad\qquad p_{j_1} = p_{j'}u_1, p_{j_2} = p_{j'}u_2 \\
\mu_{j_1} = \mu_{j'} - \frac{u_2(\sigma_{j'l}+\sigma_{j'r})}{2} \sqrt{\frac{p_{j_2}}{p_{j_1}}} \\
\mu_{j_2} = \mu_{j'} + \frac{u_2(\sigma_{j'l}+\sigma_{j'r})}{2} \sqrt{\frac{p_{j_1}}{p_{j_2}}} \\
\sigma_{j_1l}^2 = u_3(1-u_2^2)\sigma_{j'l}^2\frac{p_{j'}}{p_{j_1}} \\
\sigma_{j_1r}^2 = u_3(1-u_2^2)\sigma_{j'r}^2\frac{p_{j'}}{p_{j_1}} \\
\sigma_{j_2l}^2 = (1-u_3)(1-u_2^2)\sigma_{j'l}^2\frac{p_{j'}}{p_{j_2}} \\
\sigma_{j_2r}^2 = (1-u_3)(1-u_2^2)\sigma_{j'r}^2\frac{p_{j'}}{p_{j_2}} \qquad\quad
\label{eq:split}
\end{multline}

In order to decide whether the merge and split steps should be accepted or not, the acceptance probability \cite{Richardson1997} can be derived as follows: 

\begin{multline}
\mathcal{A}=\frac{p(\mathcal{X},Z|\Theta')}{p(\mathcal{X},Z|\Theta)}\frac{m'\mathcal{P}(m'|\lambda)}{\mathcal{P}(m|\lambda)}\frac{p_{j_1}^{\gamma-1+n_1}p_{j_2}^{\gamma-1+n_2}}{p_{j'}^{\gamma-1+n_1+n_2}Beta(\gamma,m\gamma)} \\
\times \sqrt{\frac{\kappa}{2\pi}} \exp[-\frac{1}{2}\kappa{(\mu_{j_1}-\xi)+(\mu_{j_2}-\xi)+(\mu_{j'}-\xi)}] \\
\times \frac{\beta^{\alpha}}{\Gamma(\alpha)}(\frac{\sigma_{j_1l}^2\sigma_{j_1r}^2\sigma_{j_2l}^2\sigma_{j_2r}^2}{\sigma_{j'l}^2\sigma_{j'r}^2})^{-\alpha-1} \qquad\qquad\qquad\qquad\\
\times \exp [{-\beta(\sigma_{j_1l}^2+\sigma_{j_1r}^2+\sigma_{j_2l}^2+\sigma_{j_2r}^2-\sigma_{j'l}^2-\sigma_{j'r}^2)}] \\
\qquad\times \frac{d_{m'}}{b_mP_{alloc}} [Beta(\mu_1|2,2)Beta(\mu_2|2,2)Beta(\mu_3|1,1)]^{-1} \\
\times \frac{p_{j'}|\mu_{j_1}-\mu_{j_2}|\sigma_{j_1l}^2\sigma_{j_1r}^2\sigma_{j_2l}^2\sigma_{j_2r}^2}{\mu_2(1-\mu_2^2)\mu_3(1-\mu_3)\sigma_{j'l}^2\sigma_{j'r}^2} \qquad\qquad\qquad\quad
\label{eq:acptProMS}
\end{multline}
where $\Theta'$ and $m' = m + 1$ denote the mixture parameters set and the components number respectively before merge or after split steps. $\kappa$ is a known hyperparameter and $\xi$ is the midpoint of the variation interval of the involved data observations. Besides, $P_{alloc}$ is the probability that this particular allocation is made. Therefore, the acceptance probability for merge step is $\min(1,\mathcal{A})$ and, correspondingly, for split step is $\min(1,\mathcal{A}^{-1})$.

\textbf{Birth and Death Steps}: Compared to merge and split steps, birth and death steps are relatively straightforward because the newborn and dead components are empty ones which means parameter re-calculation is not needed. Mixture weight $p_{new}$ in birth step can be obtained by sampling from Beat distribution $p_{new} \sim Beta(1,m)$ and mixture parameters can be derived from the priors as follows\cite{Casella2004}:
\begin{align}
\mu \sim \mathcal{N}(\xi,\kappa^{-1}), \quad \sigma_{l}^{-2},\sigma_{r}^{-2} \sim \Gamma(\alpha,\beta), \quad \beta \sim \Gamma(g,h)
\label{eq:prior}
\end{align}
where hyperparameters $\kappa$, $\alpha$, $g$ and $h$ are estimated by data. For death step, an empty component should be randomly selected and deleted among the existing components if there is any. Otherwise, this step will be skipped. After birth and death steps, mixture weights $p_j$ should be re-scaled so that all weights sum to 1. Acceptance probability for birth and death steps is also required as the one for merge and split steps whose definition is as follows:

\begin{multline}
\mathcal{A}'=\frac{\mathcal{P}(m'|\lambda)}{\mathcal{P}(m|\lambda)}\frac{1}{Beta(m\gamma,\gamma)}p_{j'}^{\gamma-1}(1-p_{j'})^{N+m\gamma-m}m' \\
\times \frac{d_{m'}}{(m_0+1)b_m}\frac{1}{Beta(p_{j'}|1,m)}(1-p_{j'})^m\qquad\quad
\label{eq:acptProBD}
\end{multline}
where $m_0$ is the amount of empty components. Thus, the probabilities of occurrence of birth and death steps are $\min(1,\mathcal{A}')$ and $\min(1,\mathcal{A}'^{-1})$\cite{Richardson1997}.

Finally, Figure \ref{fig:1} describes the dependencies between constants and variables involved in the Bayesian network of RJMCMC mixture parameter learning, and then, a typical learning procedure of AGM can be summarized as follows:
\bigskip


\begin{figure}[b]
\centering
\includegraphics[width=0.4\paperwidth]{xyzMerge.jpg}
\caption{(a) Original synthetic data grouping by dimensional; (b) AGM clustering results}
\label{fig:2}
\end{figure}


\begin{table*}[b]
\caption{Original NSL-KDD data records}
\begin{center}
\begin{tabular}{|c|c|}
\hline
\multicolumn{1}{|p{1cm}|}{\centering\textbf{No}} & \multicolumn{1}{|p{10cm}|}{\centering\textbf{\textit{Value}}}\\
\hline
1 & {0,tcp,ftp\_data,SF,491,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0.00,0.00,0.00,0.00,1.00,0.00,0.00,150,25,0.17,0.03,0.17,0.00,0.00,0.00,0.05,0.00,normal}\\
2 & {0,udp,other,SF,146,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,13,1,0.00,0.00,0.00,0.00,0.08,0.15,0.00,255,1,0.00,0.60,0.88,0.00,0.00,0.00,0.00,0.00,normal} \\
3 & {0,tcp,private,S0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,123,6,1.00,1.00,0.00,0.00,0.05,0.07,0.00,255,26,0.10,0.05,0.00,0.00,1.00,1.00,0.00,0.00,neptune} \\
4 & {0,tcp,http,SF,232,8153,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,5,5,0.20,0.20,0.00,0.00,1.00,0.00,0.00,30,255,1.00,0.00,0.03,0.04,0.03,0.01,0.00,0.01,normal} \\
5 & {0,tcp,http,SF,199,420,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,30,32,0.00,0.00,0.00,0.00,1.00,0.00,0.09,255,255,1.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,normal} \\
6 & {0,icmp,eco\_i,SF,18,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0.00,0.00,0.00,0.00,1.00,0.00,0.00,1,16,1.00,0.00,1.00,1.00,0.00,0.00,0.00,0.00,ipsweep} \\
\hline
\end{tabular}
\label{tab1}
\end{center}
\end{table*}

\begin{table}[b]
\caption{Accuracy Analysis ($M = 2$)}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{1}{|p{1.5cm}|}{\centering \textbf{Component number $j = 1$}} & \multicolumn{1}{|p{1.5cm}|}{\centering \textbf{Mean \\($\mu_j$)}} & \multicolumn{1}{|p{1.5cm}|}{\centering \textbf{Left standard deviation ($\sigma_{lj}$)}} & \multicolumn{1}{|p{1.5cm}|}{\centering \textbf{Right standard deviation ($\sigma_{rj}$)}}\\
\hline
$\xi$ &-30.00, 0.00, 0.00&5.00, 1.00, 1.00&1.00, 1.00, 1.00\\
$\hat{\xi}$ &-31.9, 0.05, -0.05&4.78, 0.94, 1.01&2.65, 0.92, 0.92\\
\multicolumn{1}{|p{1.5cm}|}{\centering Euclidean Distance} & 1.882 & 0.226 & 1.657 \\
\hline
\multicolumn{1}{|p{1.5cm}|}{\centering \textbf{Component number $j = 2$}} & \multicolumn{1}{|p{1.5cm}|}{\centering \textbf{Mean \\($\mu_j$)}} & \multicolumn{1}{|p{1.5cm}|}{\centering \textbf{Left standard deviation ($\sigma_{lj}$)}} & \multicolumn{1}{|p{1.5cm}|}{\centering \textbf{Right standard deviation ($\sigma_{rj}$)}}\\
\hline
$\xi$ &30.00, 0.00, 0.00&1.00, 1.00, 1.00&5.00, 1.00, 1.00\\
$\hat{\xi}$ &31.70, 0.02, -0.01&2.24, 0.95, 0.91&5.06, 0.97, 0.93\\
\multicolumn{1}{|p{1.5cm}|}{\centering Euclidean Distance} & 1.70 & 1.247 & 0.099 \\
\hline
\end{tabular}
\label{tab2}
\end{center}
\end{table}



\noindent\textbf{Input:} Data observations $\mathcal{X}$ and components number $M$ \\
\textbf{Output:} AGM mixture parameter set $\Theta$
\begin{enumerate}
\item Initialization
\item Step $t$: For $t = 1,\ldots$
\begin{enumerate}
\item[]\textbf{Gibbs sampling part}
\item Generate $Z^{(t)}$ from Eq. \eqref{eq:memVector}
\item Compute $n_j^{(t)}$ from Eq. \eqref{eq:nj}
\item Generate $p_j^{(t)}$ from Eq. \eqref{eq:posterWeight}
\item[] \textbf{Metropolis-Hastings part}
\item Sample $\xi_j^{(t)}$ ($\mu_j^{(t)}, \sigma_{lj}^{(t)}, \sigma_{rj}^{(t)}$) from Eqs. \eqref{eq:posters}
\item Compute acceptance ratio $r$ from Eq. \eqref{eq:r}
\item Generate $\alpha = min[1,r]$ and $u \sim U_{[0,1]}$
\item If $\alpha \geq u$ then $\xi^{(t)} = \xi^{(t-1)}$
\item[] \textbf{RJMCMC part}
\item Generate $u' \sim U_{[0,1]}$. If $b_m>=u'$, perform split or birth step, then calculate acceptance probability $\mathcal{A}$. If the step is accepted, set $m=m+1$.
\item Generate $u' \sim U_{[0,1]}$. If $d_m>=u'$, perform merge or death step, then calculate acceptance probability $\mathcal{A}'$. If the step is accepted, set $m=m-1$.
\end{enumerate}
\end{enumerate}



\subsection{Model Selection}
Theoretically, RJMCMC learning process should always be able to derive the optimal components number $M$. However, because of the stochastic sampling, improper proposal distributions or bad initialization parameters, learning result based on a single estimation run is not always satisfactory. In order to establish a robust parameter estimation algorithm, we evaluate the estimation outputs derived from multiple RJMCMC runs with different initial values of components number by calculating their marginal likelihood with the Laplace approximation\cite{Bouguila2009} on the logarithm scale which is defined as follows:

\begin{multline}
\log(p(\mathcal{X}|M)) = \log(p(\mathcal{X}|\hat{\Theta},M)) + \log(\pi(\hat{\Theta}|M)) \\
 + \frac{N_p}{2}\log(2\pi)+\frac{1}{2}\log(|H(\hat{\Theta})|)\qquad\quad
\label{eq:margLikeli}
\end{multline}
where $\hat{\Theta}$ denotes the proposed optimal parameter set derived from a specific learning process and $\pi(\hat{\Theta}|M)$ is the prior density of mixture parameters as well as its Hessian matrix $H(\hat{\Theta})$ which is asymptotically equal to the posterior variance matrix.

\section{Experimental results}
\subsection{Synthetic Data}
Before testing the AGM model against real application datasets, a feasibility verification step is achieved against a 3-dimensional synthetic dataset having 400 observations composed of clusters and the iteration limite is set to 300. Hyperparameters related to the calculations of mixture weights and parameters are set as $\gamma_j = 1$ and both $\eta$ and $\tau$ are considered as $d$-dimensional zero vectors. Consequently, hyperparameters for priors of mixture parameters are set as follows\cite{Stephens2000}:
\begin{align}
\kappa = \frac{1}{\mathcal{R}^2}, \quad \alpha = 3, \quad g=0.3, \quad h=\frac{100g}{\alpha\mathcal{R}^2}
\label{eq:hypers}
\end{align}
where $\mathcal{R}$ is the interval of variation of observations.

Figure \ref{fig:2} compares original grouping and RJMCMC learning results by dimensional and Table \ref{tab2} represents initial and estimated mixture parameters given components number $M=2$. Moreover, The Euclidean distances are calculated to evaluate the accuracy of the AGM model. Based on synthetic test results, AGM model demonstrated promising unsupervised clustering by providing satisfactory deviation and grouping accuracy.

\subsection{Intrusion Detection}
\begin{table}[b]
\caption{Translation and Normalization of Internet Protocols (Enumerated Values)}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\multicolumn{1}{|p{2cm}|}{\centering \textbf{Internet Protocols}} & \multicolumn{1}{|p{2cm}|}{\centering \textbf{\textit{Number of Occurrences}}} & \multicolumn{1}{|p{2cm}|}{\centering \textbf{\textit{Normalized Values}}}\\
\hline
ICMP & 1655 & 0\\
UDP & 3011 & 0.071867 \\
TCP & 20526 & 1 \\
\hline
\end{tabular}
\label{tab3}
\end{center}
\end{table}

\begin{table}[b]
\caption{AGM Statistics}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{1}{|p{1.5cm}|}{\centering \textbf{Init. Comp. Number}} & \multicolumn{1}{|p{1.5cm}|}{\centering \textbf{\textit{Estimated Comp. Number}}} & \multicolumn{1}{|p{1.5cm}|}{\centering \textbf{\textit{Accuracy}}} & \multicolumn{1}{|p{1.5cm}|}{\centering \textbf{\textit{Marginal Likelihood}}}\\
\hline
1 &2& 60.86\% & $-1.3596\mathrm{e}{6}$\\
2 &2& 52.34\% & $-1.9886\mathrm{e}{6}$ \\
3 &2& 53.39\% & $-2.2218\mathrm{e}{6}$ \\
\hline
\end{tabular}
\label{tab4}
\end{center}
\end{table}

\begin{table}[b]
\caption{Confusion Matrices and Statistics of GMM and AGM}
\begin{center}

\begin{minipage}{0.75\textwidth}
\begin{flushleft}
\begin{tabular}{cc}

\begin{minipage}{.3\textwidth} 
\begin{center}
\textbf{GMM} \\
\begin{tabular}{|c|c|c|}
\hline
 & \multicolumn{1}{|p{.7cm}|}{\centering \textbf{\textit{NF $^{\mathrm{a}}$}}} & \multicolumn{1}{|p{.7cm}|}{\centering \textbf{\textit{F $^{\mathrm{b}}$}}}\\
\hline
\multicolumn{1}{|p{.7cm}|}{\centering \textbf{\textit{NF}}} & 4238 & 7505\\
\multicolumn{1}{|p{.7cm}|}{\centering \textbf{\textit{F}}} & 3397 & 10052\\
\hline
\end{tabular}
\end{center}
\end{minipage} &

\begin{minipage}{.3\textwidth}    
\begin{center}
\textbf{AGM} \\
\begin{tabular}{|c|c|c|}
\hline
 & \multicolumn{1}{|p{.7cm}|}{\centering \textbf{\textit{NF}}} & \multicolumn{1}{|p{.7cm}|}{\centering \textbf{\textit{F}}}\\
\hline
\multicolumn{1}{|p{.7cm}|}{\centering \textbf{\textit{NF}}} & 2456 & 9278\\
\multicolumn{1}{|p{.7cm}|}{\centering \textbf{\textit{F}}} & 582 & 12867\\
\hline
\end{tabular}
\end{center}
\end{minipage}  

\end{tabular}
\end{flushleft}
\end{minipage}
\end{center}

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
 & \multicolumn{1}{|p{1.5cm}|}{\centering \textbf{\textit{GMM}}} & \multicolumn{1}{|p{1.5cm}|}{\centering \textbf{\textit{AGM}}}\\
\hline
\multicolumn{1}{|p{2.5cm}|}{\centering \textbf{\textit{Accuracy}}}  & 53.39\% & 60.86\%\\
\multicolumn{1}{|p{2.5cm}|}{\centering \textbf{\textit{Precision}}} & 36.09\% & 20.93\%\\
\multicolumn{1}{|p{2.5cm}|}{\centering \textbf{\textit{False Positive Rate}}}  & 42.75\% & 41.90\%\\
\multicolumn{1}{|p{2.5cm}|}{\centering \textbf{\textit{False Negative Rate}}} & 44.49\% & 19.16\%\\
\hline
\multicolumn{3}{l}{$^{\mathrm{a}}$Non fault-prone, $^{\mathrm{b}}$Fault-prone.}
\end{tabular}
\end{center}
\label{tab5}
\end{table}

A challenging NSL-KDD'99 intrusion detection database\cite{Tavallaee2009} is selected as our target application which was used for the Third International Knowledge Discovery and Data Mining Tools Competition. In order to simplify the testing procedure and improve the fitting accuracy, we choose 20\% of the whole database containing 25192 records clustered into two groups with 11743 intrusions and 13449 normal behaviors. Each data instance consists of 41 attributes (38 continuous or discrete numerical and 3 symbolic)(Table \ref{tab1}). All the instances are grouped as either normal or attacks including DoS, Prob, U2R and R2L. Before applying our test model to the database, we need to translate non-numerical attributes to numerical and then, normalize records properly to ensure an accurate result. Therefore, enumerated and discrete values are substituted by their number of appearances in the whole database which could reflect their density distribution. Then, we apply feature scaling method to normalize numerical attributes to the range between 0 and 1 as follows: 

\begin{align}
x' = \frac{x - min(x)}{max(x) - min(x)}
\label{eq:normalize}
\end{align}
where $x$ and $x'$ are the attribute values before and after normalization as well as the maximum and minimum values $max(x)$  and $min(x)$. An example of normalizing Internet Protocol attribute values is given in Table \ref{tab3}. In this way, we could apply unified proposal distribution for every dimension using universal value of hyperparameter $\Sigma$ during the random walk MCMC sampling.

We deploy our AGM model to the dataset with initial components number set between 1 and 3 and the learning iteration limit set to 30 loops. To better evaluate the performance and accuracy of our model under different initial number of components, at the end of each estimation, the fitting accuracy and marginal likelihood\cite{Bouguila2009} are recorded into Table \ref{tab4}. Obviously, the best result is the first run with initial components number equals to 1 which has the largest marginal likelihood value $-1.3596\mathrm{e}{6}$ and accuracy percentage 60.86\%. In order to better evaluate the model performance, the compare specifications derived from confusion matrices of AGM and GGM are shown in Table \ref{tab5}. In general, conpared to GMM, AGM performs in a more accurate way for intrusion detection in terms of higher accuracy and much lower False Negative Rate indicating that AGM causes less false alarms of attack behaviors. However, it has a similar False Positive Rate and a lower precision value which means the intrusion detection abilities of both models are not satisfactory because of the absence of feature selection. In addition, high-dimensional database will dramatically increase the noise during the clustering and eventually affect the usability of the model for real applications. Therefore, data-oriented model adjustment and dimensionality reduction techniques should be involved in order to mitigate this problem and achieve better detection outcomes.

\section{Conclusion and Future Work}
A novel Bayesian framework based on asymmetric Gaussian mixture model and reversible jump MCMC is proposed to address intrusion detection problem. RJMCMC reinforced classic MCMC methodology by introducing extra four steps of split, merge, birth and death which enabled the transfer between AGM models with different components numbers and, consequently, different mixture parameters. Therefore, it improves the flexibility and generality of the learning process. A horizontal comparison with Gaussian mixture model is made to show the merits of this model. The results reveal the fact that AGM outperforms GMM from most statistical specifications. However, the performances of both models are not satisfactory especially for high-dimensional datasets which implies further model adjustments and improvements are necessary. 

\section*{Appendix A}
\subsection{Derivation of Acceptance Ratio $r$ by Eq. \eqref{eq:r}}
The derivation of acceptance ratio $r$ is based on the assumption that mixture parameters are independent from each other which means that:
\bigskip
\begin{multline}
\qquad\qquad\pi(\Theta) = \pi(p,\xi) = \pi(\xi) \\
= \prod_{j=1}^M\pi(\mu_j)\pi(\sigma_{lj})\pi(\sigma_{rj}) \qquad\qquad\qquad\qquad\qquad\\
= \prod_{j=1}^M\mathcal{N}_d(\mu_j|\eta,\Sigma)\mathcal{N}_d(\sigma_{lj}|\tau,\Sigma)\mathcal{N}_d(\sigma_{rj}|\tau,\Sigma)\qquad\qquad\qquad
\label{eq:14}
\end{multline}
in Eq. \eqref{eq:14}, since the mixture weigh $p$ is generated following Gibbs sampling method whose acceptance ratio is always 1, it should be excluded from Metropolis-Hastings estimation step. Accordingly, apply the same rule to the proposal distribution as well:
\begin{multline}
q(\Theta^{(t)}|\Theta^{(t-1)}) = q(\xi^{(t)}|\xi^{(t-1)})= \\
\prod_{j=1}^M\mathcal{N}_d(\mu_j^{(t)}|\mu_j^{(t-1)},\Sigma)\mathcal{N}_d(\sigma_{lj}^{(t)}|\sigma_{lj}^{(t-1)},\Sigma)\mathcal{N}_d(\sigma_{rj}^{(t)}|\sigma_{rj}^{(t-1)},\Sigma)
\label{eq:15}
\end{multline}
by combining Eqs. \eqref{eq:pdf} \eqref{eq:compPdf} \eqref{eq:posters} \eqref{eq:14} and \eqref{eq:15}, equation \eqref{eq:r} can be written as follows:

\begin{multline}
\qquad\qquad r = \frac{p(\mathcal{X}|\Theta^{(t)})\pi(\Theta^{(t)})q(\Theta^{(t-1)}|\Theta^{(t)})}{p(\mathcal{X}|\Theta^{(t-1)})\pi(\Theta^{(t-1)})q(\Theta^{(t)}|\Theta^{(t-1)})} \\
= \prod_{i=i}^N \prod_{j=1}^M(\frac{p(X_i|\mu_j^{(t)},\sigma_{lj}^{(t)},\sigma_{rj}^{(t)})}
{p(X_i|\mu_j^{(t-1)},\sigma_{lj}^{(t-1)},\sigma_{rj}^{(t-1)})} \qquad\qquad\\
\times \frac{\mathcal{N}_d(\mu_j^{(t)}|\eta,\Sigma)\mathcal{N}_d(\sigma_{lj}^{(t)}|\tau,\Sigma)\mathcal{N}_d(\sigma_{rj}^{(t)}|\tau,\Sigma)}{\mathcal{N}_d(\mu_j^{(t-1)}|\eta,\Sigma)\mathcal{N}_d(\sigma_{lj}^{(t-1)}|\tau,\Sigma)\mathcal{N}_d(\sigma_{rj}^{(t-1)}|\tau,\Sigma)} \\
\times \frac{\mathcal{N}_d(\mu_j^{(t-1)}|\mu_j^{(t)},\Sigma)\mathcal{N}_d(\sigma_{lj}^{(t-1)}|\sigma_{lj}^{(t)},\Sigma)\mathcal{N}_d(\sigma_{rj}^{(t-1)}|\sigma_{rj}^{(t)},\Sigma)}{\mathcal{N}_d(\mu_j^{(t)}|\mu_j^{(t-1)},\Sigma)\mathcal{N}_d(\sigma_{lj}^{(t)}|\sigma_{lj}^{(t-1)},\Sigma)\mathcal{N}_d(\sigma_{rj}^{(t)}|\sigma_{rj}^{(t-1)},\Sigma)}
\label{eq:16}
\end{multline}

\section*{Acknowledgment}
The completion of this research work was made possible thanks to Concordia University via a Concordia University Research Chair Tier II.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,shuai}

\end{document}
